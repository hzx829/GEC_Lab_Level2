{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 2: Lab 3\n",
    "\n",
    "#### In this lab we will cover:\n",
    "\n",
    "- Support Vector Machine, Softmax for image recognition\n",
    "- Neural Networks with TensorFlow: Setting up Architecture and Loss\n",
    "    - Architecture, activation functions\n",
    "    - loss functions, weight initialization, \n",
    "    - batch normalization, regularization (L2/dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from skimage.feature import hog\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn import ensemble\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import skimage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-image.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://opencv.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Support Vector Machine\n",
    "\n",
    "Support Vector Machine is a supervised classification model (sometimes we also use Support Vector Regression for a regression problem). In short, the goal of SVM is to find a **classifier** in the feature space to classify the positive samples and negtive samples. \n",
    "\n",
    "###### How to use SVM in image classification?\n",
    "\n",
    "To conduct an SVM over the image dataset, we need to extract features from the images first. \n",
    "Here are some common options to extract the features:\n",
    "1. *Conduct a **PCA** over all the pixels*\n",
    "2. *Scale-invariant feature transform ( **SIFT** )*\n",
    "3. *Histogram of Oriented Gradient ( **HOG** )*\n",
    "\n",
    "In the following example, we use *HOG* to classify the images in the ***Dogs v.s. Cats*** dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/dogs-vs-cats/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=\"/Users/shahab/Documents/kaggle/dogs_vs_cats\"\n",
    "img_list = os.listdir(data_path+\"/train/\")\n",
    "len(img_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 25000 images. Their labels are shown in the file names. Now we define 'cat'=1 and 'dog'=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [int(img.split('.')[0]=='cat') for img in img_list]\n",
    "labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [mpimg.imread(data_path +'/train/'+ img) for img in img_list[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "for i in range(10):\n",
    "    plt.subplot(2,5,i+1)\n",
    "    plt.imshow(images[i])\n",
    "    plt.title(labels[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since not every image is in the same size, let's resize those images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lbl2name={0:'dog',1:'cat'}\n",
    "lbl2count={0:0,1:0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RESIZED_IMAGES_AVAILABLE=os.path.exists(data_path+\"/train_resized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not RESIZED_IMAGES_AVAILABLE:\n",
    "    print(\"Resizing images...\")\n",
    "    resized_images = [skimage.transform.resize(img,(128,128)) for img in images]\n",
    "    print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not RESIZED_IMAGES_AVAILABLE:\n",
    "    os.makedirs(data_path+\"/train_resized\")\n",
    "    print(\"Saving resized images...\")\n",
    "    #save resized images to save time for later runs\n",
    "    file_path=data_path+\"/train_resized/\"\n",
    "    for im,lbl in zip(resized_images,labels):\n",
    "        file_name=\"%s.%d.jpg\"%(lbl2name[lbl],lbl2count[lbl])\n",
    "        mpimg.imsave(file_path+file_name, im, format=\"jpg\")\n",
    "        lbl2count[lbl]+=1\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RESIZED_IMAGES_AVAILABLE:\n",
    "    #read resized images\n",
    "    print(\"Loading resized images...\")\n",
    "    file_path=data_path+\"/train_resized/\"\n",
    "    img_list = os.listdir(file_path)\n",
    "    resized_images_copy = []\n",
    "    labels_copy=[]\n",
    "    for img in tqdm(img_list):\n",
    "        if not img.endswith(\"jpg\"):\n",
    "            continue\n",
    "        resized_images_copy.append(mpimg.imread(file_path + img))\n",
    "        labels_copy.append(int(img.split('.')[0]=='cat'))\n",
    "    print(\"Done.\")\n",
    "    print(\"Number of images loaded:\",len(resized_images_copy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_copy[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first zero should appear right in the middle of the array. \n",
    "Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.index(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a small sample of this data to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dog_start_idx=labels_copy.index(0)\n",
    "n_samples=100\n",
    "cat_idx=np.random.randint(0,dog_start_idx, size=n_samples)\n",
    "dog_idx=np.random.randint(dog_start_idx, len(labels_copy), size=n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert(sum([labels[i] for i in cat_idx])==n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert(sum([labels[i] for i in dog_idx])==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resized_images=[resized_images_copy[i] for i in cat_idx]+[resized_images_copy[i] for i in dog_idx]\n",
    "labels=[labels_copy[i] for i in cat_idx]+[labels_copy[i] for i in dog_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(resized_images), len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show 10 random images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "for i, idx in enumerate(np.random.randint(0,len(resized_images),size=10)):\n",
    "    plt.subplot(2,5,i+1)\n",
    "    plt.imshow(resized_images[idx])\n",
    "    plt.title(labels[idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of Oriented Gradients (HOG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-image.org/docs/dev/auto_examples/features_detection/plot_hog.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try HOG on those resized images. A typical HOG contain the following steps:\n",
    "1. *converting an rgb image to a grey image*\n",
    "2. *(optional) global image normalisation*\n",
    "3. *computing the gradient image in x and y*\n",
    "4. *computing gradient histograms*\n",
    "5. *normalising across blocks*\n",
    "6. *flattening into a feature vector*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a random image and try HOG on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i=np.random.randint(len(resized_images))\n",
    "img=resized_images[i]\n",
    "lbl=labels[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(skimage.color.rgb2gray(img), cmap=plt.cm.gray);\n",
    "plt.title(lbl2name[lbl])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play with orientations, pixels_per_cell parameters to see the effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, im = hog(skimage.color.rgb2gray(img), orientations=9,\n",
    "           pixels_per_cell=(8, 8), cells_per_block=(1, 1),\n",
    "           visualise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(im, cmap=plt.cm.gray_r);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def HOG(img):\n",
    "    #img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY )    \n",
    "    img = skimage.color.rgb2gray(img)\n",
    "    return hog(img,orientations=8, pixels_per_cell=(8, 8),cells_per_block=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_feature=[]\n",
    "for img in tqdm(resized_images):\n",
    "    #print '.',\n",
    "    img_feature.append(HOG(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(img_feature[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have extracted above features from each image. \n",
    "The number could be changed by setting different parameters in hog.\n",
    "\n",
    "Now we can begin our SVM training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(img_feature, labels, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(C=100)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = ensemble.RandomForestClassifier(n_estimators=10, n_jobs=10, max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using HOG and SVM we achieve the above accuracy on the test set. You can also try some other methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Softmax\n",
    "Softmax is usually used as a activation function in NN, especially when we use Neural Networks to solve multi-class classification problems.  generalization of the logistic function that \"squashes\" a K-dimensional vector $\\displaystyle \\mathbf {z} $  of arbitrary real values to a K-dimensional vector $\\displaystyle \\sigma (\\mathbf {z} )$  of real values in the range [0, 1] that add up to 1.  The function is given by ![function 1][1]\n",
    "[1]: https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3\n",
    "\n",
    "As a result, the largest value in the vector is mapped to a value approximates to 1 and the other values are mapped to values approximate to 0.\n",
    "Here is an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two different implementations are shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax1(z):\n",
    "    y = np.zeros(len(z))\n",
    "    for i in range(len(z)):\n",
    "        y[i] = np.exp(z[i])\n",
    "    return y/np.sum(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax2(x,t=1):\n",
    "    # x should be 2d: array of vectors\n",
    "    x1=x-np.max(x,axis=1, keepdims=True)\n",
    "    x2=np.exp(x1/t)\n",
    "    x3=x2/np.sum(x2, axis=1, keepdims=True)\n",
    "    return x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = np.array([3, 1, -3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax1(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax1(z*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax2([z*1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below can illustrate the function better: \n",
    "<img src=\"https://pic3.zhimg.com/v2-998ddf16795db98b980443db952731c2_r.jpg\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a multi-class classification problem, we expect the Neural Networks to output a probability distribution so that we can choose the most probable class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. The Neural Networks\n",
    "We first take a look at a basic neural network:\n",
    "\n",
    "Architecture of a Neural Network             | Architecture of a single Neural\n",
    ":-------------------------:|:-------------------------:\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/300px-Colored_neural_network.svg.png\" width=\"80%\">| <img src=\"http://cs231n.github.io/assets/nn1/neuron_model.jpeg \" width=\"70%\">\n",
    "\n",
    "For one way to realize a image classifier with NN, we will treat all the pixels of each image as an input. The simplest neural network (without hidden layers) looks like this:\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/softmax-regression-scalargraph.png\" width=\"40%\">\n",
    "\n",
    "If we write that out as a equation with matrix, we get:\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/softmax-regression-vectorequation.png\" width=\"40%\">\n",
    "\n",
    "The parameters we want to optimize by gradient descending are the matrix **W** (weights) and the vector **b** (biases). Then we can realize the model with tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Activation functions\n",
    "Besides the softmax in the output layer, we alse need other activation functions to give NN the ability of non-linear model describing. Common activation functions include:\n",
    "\n",
    "sigmoid  | tanh | relu | leaky-relu \n",
    ":-------:|:-------:|:-------:|:-------:\n",
    "![](http://cs231n.github.io/assets/nn1/sigmoid.jpeg)|![](http://cs231n.github.io/assets/nn1/tanh.jpeg)|![](http://cs231n.github.io/assets/nn1/relu.jpeg)|<img src=\"http://oexh9rdj7.bkt.clouddn.com/myBlog/posts/NN/LeakyReLU.png\" width=\"70%\">\n",
    "\n",
    "In most of time, we prefer to use relu because sometimes sigmoid and tanh may get satuated and the gradient will get very close to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Then we can realize the model with tensorflow.\n",
    "\n",
    "We start with a simple structure with two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!gpustat -cup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "learning_rate = 0.001\n",
    "n_hidden1 = 256\n",
    "n_hidden2 = 32\n",
    "training_iters = 200\n",
    "display_step = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_hot_labels=pd.get_dummies(labels).values\n",
    "train_x, test_x, train_y, test_y = train_test_split(resized_images, one_hot_labels, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_x), len(train_y), len(test_x), len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x[0].shape, train_y[0].shape, test_x[0].shape, test_y[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess=tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create placeholders for the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder('float', [None, 128, 128, 3])\n",
    "y = tf.placeholder('float', [None, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_layer = tf.reshape(X, [-1, 128*128*3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Weight and Bias variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'hidden1': tf.Variable(tf.random_normal([?,?])),\n",
    "    'hidden2': tf.Variable(tf.random_normal([?,?])),\n",
    "    'out': tf.Variable(tf.random_normal([?, 2]))\n",
    "    }\n",
    "biases = {\n",
    "    'hidden1': tf.Variable(tf.random_normal([?])),\n",
    "    'hidden2': tf.Variable(tf.random_normal([?])),\n",
    "    'out': tf.Variable(tf.random_normal([2]))\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the network structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden1 = tf.nn.relu(tf.matmul(?, ?)+?)\n",
    "hidden2 = tf.nn.relu(tf.matmul(?, ?)+?)\n",
    "out = tf.matmul(?, ?)+biases['out']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights['hidden1'].eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct = tf.equal(tf.argmax(out,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy.eval({X:train_x, y:train_y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=?, logits=?))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "# sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's manually optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_idx = np.random.choice(range(len(train_x)), batch_size)\n",
    "batch_x = np.array(train_x)[batch_idx]\n",
    "batch_y = np.array(train_y)[batch_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(optimizer, feed_dict=?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights['hidden1'].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "out.eval(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy.eval(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy.eval(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tb_log_dir=\"./tflogs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if tf.gfile.Exists(tb_log_dir):\n",
    "    tf.gfile.DeleteRecursively(tb_log_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.get_default_graph().as_graph_def()\n",
    "summary_writer = tf.summary.FileWriter(tb_log_dir,graph)\n",
    "# summary_writer = tf.summary.FileWriter('.', sess.graph)\n",
    "summary_writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run tensorboard server:\n",
    "```\n",
    "tensorboard  --logdir=[use path you used in summary_writer above]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open this page: http://localhost:6006/#graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put this steps together in a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess=tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sess.run(init)\n",
    "for step in range(training_iters):\n",
    "        #take a [random] batch\n",
    "        batch_idx = np.random.choice(range(len(train_x)), batch_size)\n",
    "        batch_x = np.array(train_x)[batch_idx]\n",
    "        batch_y = np.array(train_y)[batch_idx]\n",
    "        \n",
    "        #run optimization step\n",
    "        #?   \n",
    "        \n",
    "        #this is for tensorboard\n",
    "        tf.summary.histogram('output', out)\n",
    "        tf.summary.scalar('loss', loss)\n",
    "        tf.summary.scalar('train_accuracy', accuracy)\n",
    "\n",
    "        #display progress\n",
    "        if (step+1)%display_step == 0:\n",
    "            b_loss, b_acc = sess.run([loss, accuracy], feed_dict={X: batch_x, y: batch_y})\n",
    "            t_loss, t_acc = sess.run([loss, accuracy], feed_dict={X: test_x, y: test_y})\n",
    "            print('Iter '+str(step+1)+', batch loss = '+str(b_loss)+', bacth accuracy = '+str(b_acc)+\n",
    "                 ', test loss = '+str(t_loss)+', test accuracy = '+str(t_acc))\n",
    "            \n",
    "            summary = tf.summary.merge_all()\n",
    "\n",
    "            summary_str = sess.run(summary, feed_dict={X: batch_x, y: batch_y})\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "            \n",
    "summary_writer.flush()    \n",
    "print('Training finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training a NN, our first goal is make the NN proper and complicated enough to describe a complicated problem (To solve the underfitting problem). (e.g. get the batch accuracy up to 0.90 and the test accuracy at some 0.60.) After that, we should solve the overfitting problem.\n",
    "\n",
    "In the past training, we got the loss down to 0.69 on a batch. But it is far from small enough. We may include more hidden layers or convolutional layers in our network.\n",
    "\n",
    "<span style=\"color:blue\">Let's assume our model have achieved a very high score on the trainset but a raletively low score on the testset.</span> What should we do now?\n",
    "\n",
    "##### To deal with overfitting\n",
    "There are several ways to do that.\n",
    "\n",
    "* **Regularization (L2)**\n",
    "\n",
    "L2 Regularization keeps the parameters (weights, biases, etc.) from being too large by adding a penalty of large parameters (L2 norm) in the loss function. \n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg_lambda = 0.01\n",
    "regularizer = tf.nn.l2_loss(weights['hidden1'])\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=out))+reg_lambda*regularizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to continue adjusting the reg_lambda to make a trade-off between overfitting and underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Regularization (Dropout)**\n",
    "\n",
    "Dropout will randomly disable the output of a proportion of neurals. It also keeps the parameters (weights, biases, etc.) from being too large (if some neurals have large parameters, once they are disabled, the loss will get very high).\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_layer = tf.reshape(X, [-1, 128*128*3])\n",
    "hidden1 = tf.nn.relu(tf.matmul(input_layer, weights['hidden1'])+biases['hidden1'])\n",
    "hidden1 = tf.nn.dropout(hidden1, keep_prob=0.8) # 20% of the neurals will be dropout\n",
    "hidden2 = tf.nn.relu(tf.matmul(hidden1, weights['hidden2'])+biases['hidden2'])\n",
    "hidden2 = tf.nn.dropout(hidden2, keep_prob=0.8)\n",
    "out = tf.matmul(hidden2, weights['out'])+biases['out']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Batch Normalization**\n",
    "\n",
    "Batch Normalization means to shift inputs to zero-mean and unit variance for each batch. It solves a problem called internal covariate shift.\n",
    "\n",
    "Internal covariate shift refers to covariate shift occurring within a neural network, i.e. going from layer 2 to layer 3. This happens because, as the network learns and the weights are updated, the distribution of outputs of a specific layer in the network changes. This forces the higher layers to adapt to that drift, which slows down learning.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_layer = tf.reshape(X, [-1, 128*128*3])\n",
    "hidden1 = tf.nn.relu(tf.matmul(input_layer, weights['hidden1'])+biases['hidden1'])\n",
    "hidden1 = tf.nn.dropout(hidden1, keep_prob=0.8) # 20% of the neurals will be dropout\n",
    "hidden1 = tf.contrib.layers.batch_norm(hidden1)\n",
    "hidden2 = tf.nn.relu(tf.matmul(hidden1, weights['hidden2'])+biases['hidden2'])\n",
    "hidden2 = tf.nn.dropout(hidden2, keep_prob=0.8)\n",
    "hidden2 = tf.contrib.layers.batch_norm(hidden2)\n",
    "out = tf.matmul(hidden2, weights['out'])+biases['out']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: How can we use Keras for this problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(5, 3, 3, activation='relu', input_shape=(128,128,3)))\n",
    "model.add(Convolution2D(7, 3, 3, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(X.reshape(len(X),28,28,1), y, \n",
    "          batch_size=32, nb_epoch=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: How can we use HOG features in our NN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
